{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_HW2_Transformer_2Step_Final.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Qi1F-JqStyAz"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhgCtaaM4dqN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "706d522a-93ae-43a0-8d91-aa7c76d6ca97"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0PaE_FxLe5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = '/content/drive/My Drive/NLP_HW2/Dataset'\n",
        "general_path = '/content/drive/My Drive/NLP_HW2'\n",
        "\n",
        "train_path = os.path.join(data_path, \"train.json\")\n",
        "test_path = os.path.join(data_path, \"test.json\")\n",
        "dev_path = os.path.join(data_path, \"dev.json\")\n",
        "glove_path = os.path.join(general_path, \"glove.6B.50d.txt\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lbVF0ehMX8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "device = \"cuda\"\n",
        "PAD_TOKEN = '<pad>'\n",
        "UNK_TOKEN = '<unk>'\n",
        "\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "# torch.autograd.set_detect_anomaly(True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8krnCYMMgOb",
        "colab_type": "text"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW4rC0KvYyAM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_dataset(path: str):\n",
        "  with open(path) as f:\n",
        "      dataset = json.load(f)\n",
        "  \n",
        "  sentences, labels = {}, {}\n",
        "  for sentence_id, sentence in dataset.items():\n",
        "      sentence_id = int(sentence_id)\n",
        "      sentences[sentence_id] = {\n",
        "          'words': sentence['words'],\n",
        "          'lemmas': sentence['lemmas'],\n",
        "          'pos_tags': sentence['pos_tags'],\n",
        "          'dependency_heads': [int(head) for head in sentence['dependency_heads']],\n",
        "          'dependency_relations': sentence['dependency_relations'],\n",
        "          'predicates': sentence['predicates'],\n",
        "      }\n",
        "\n",
        "      labels[sentence_id] = {\n",
        "          'predicates': sentence['predicates'],\n",
        "          'roles': {int(p): r for p, r in sentence['roles'].items()}\n",
        "      }\n",
        "\n",
        "  return sentences, labels"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4atlA9FMMeZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_sentences, train_labels = read_dataset(train_path)\n",
        "dev_sentences, dev_labels = read_dataset(dev_path)\n",
        "test_sentences, test_labels = read_dataset(test_path)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rja8GYaKinll",
        "colab_type": "text"
      },
      "source": [
        "### Single Predicate Converter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kymKe6_2RitR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function is for Train and Dev dataset\n",
        "# It gets sentences and labels in standard format\n",
        "# Returns all sentences in single predicate format with new structre\n",
        "def single_predicate_converter(sentences, labels):\n",
        "  new_sentences = []\n",
        "  for k, v in sentences.items():\n",
        "    for pred_indx in labels[k]['roles'].keys():\n",
        "\n",
        "      # Create a predicate mask for multi-predicate sentences\n",
        "      # for example: - , EAT/BITE , - , DRINK --> - , - , - , DRINK \n",
        "      new_pred = ['_']*len(labels[k]['predicates'])\n",
        "      new_pred[pred_indx] = labels[k]['predicates'][pred_indx]\n",
        "\n",
        "      #create label indicator feature for argument identification\n",
        "      new_role = ['!_' if role != '_' else '_' for role in labels[k]['roles'][pred_indx]]\n",
        "\n",
        "      # create predicate indicator feature\n",
        "      # for example: the cat ate the fish --> 0, 0, 1, 0, 0\n",
        "      predicate_indicator = [0]*len(labels[k]['predicates'])\n",
        "      predicate_indicator[pred_indx] = 1 \n",
        "\n",
        "      # create word indicator feature in respect to predicate position\n",
        "      # for example: the cat ate the fish --> -2, -1, 0, 1, 2\n",
        "      lemmas_indicator = [0]*len(sentences[k]['predicates'])\n",
        "      for i, x in enumerate(predicate_indicator):\n",
        "        lemmas_indicator[i] = i - pred_indx\n",
        "\n",
        "      new_sentences.append({\n",
        "        'sentence_id': k,\n",
        "        'position_predicate': pred_indx,\n",
        "        'lemmas': sentences[k]['lemmas'],\n",
        "        'pos_tags': sentences[k]['pos_tags'],\n",
        "        'predicate': new_pred,\n",
        "        'predicate_indicator': predicate_indicator,\n",
        "        'lemmas_indicator': lemmas_indicator,\n",
        "        'roles': labels[k]['roles'][pred_indx],\n",
        "        'bi_roles': new_role\n",
        "      })\n",
        "\n",
        "  return new_sentences"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gRhezYK-30b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This converter is for Test dataset\n",
        "# In test dataset we don't have access to the labels\n",
        "# It gets test setenctes in standard format (Different from TA code)\n",
        "# It gets all test setences at once\n",
        "# Returns test sentences in new format with more features\n",
        "def test_dataset_single_predicate_converter(sentences):\n",
        "\n",
        "  new_sentences = []\n",
        "\n",
        "  for k, v in sentences.items():\n",
        "\n",
        "    # Extract the predicates index for each sentence\n",
        "    predicate_indexes = []\n",
        "    for indx, item in enumerate(sentences[k]['predicates']):\n",
        "      if item != '_':\n",
        "        predicate_indexes.append(indx)\n",
        "    \n",
        "    for pred_indx in predicate_indexes:\n",
        "\n",
        "      # Create a predicate mask for multi-predicate sentences\n",
        "      # for example: - , EAT/BITE , - , DRINK --> - , - , - , DRINK \n",
        "      new_pred = ['_']*len(sentences[k]['predicates'])\n",
        "      new_pred[pred_indx] = sentences[k]['predicates'][pred_indx]\n",
        "\n",
        "      # create predicate indicator feature\n",
        "      # for example: the cat ate the fish --> 0, 0, 1, 0, 0\n",
        "      predicate_indicator = [0]*len(sentences[k]['predicates'])\n",
        "      predicate_indicator[pred_indx] = 1 \n",
        "\n",
        "      # create word indicator feature in respect to predicate position\n",
        "      # for example: the cat ate the fish --> -2, -1, 0, 1, 2\n",
        "      lemmas_indicator = [0]*len(sentences[k]['predicates'])\n",
        "      for i, x in enumerate(predicate_indicator):\n",
        "        lemmas_indicator[i] = i - pred_indx\n",
        "\n",
        "\n",
        "      new_sentences.append({\n",
        "        'sentence_id': k,\n",
        "        'position_predicate': pred_indx,\n",
        "        'lemmas': sentences[k]['lemmas'],\n",
        "        'pos_tags': sentences[k]['pos_tags'],\n",
        "        'predicate': new_pred,\n",
        "        'predicate_indicator': predicate_indicator,\n",
        "        'lemmas_indicator' : lemmas_indicator\n",
        "      })\n",
        "\n",
        "  return new_sentences"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UDNZ8ld25me",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_single_pred_sentences = single_predicate_converter(train_sentences, train_labels)\n",
        "dev_single_pred_sentences = single_predicate_converter(dev_sentences, dev_labels)\n",
        "test_single_pred_sentences = test_dataset_single_predicate_converter(test_sentences)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3UXVX8MIkzN",
        "colab_type": "text"
      },
      "source": [
        "### Create Dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM3Je4t0OxDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab2ids = {\n",
        "    'lemmas': {},\n",
        "    'pos_tags': {},\n",
        "    'predicate': {},\n",
        "    'roles': {},\n",
        "    'predicate_indicator': {},\n",
        "    'lemmas_indicator': {},\n",
        "    'bi_roles': {}\n",
        "}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCQIhK7aI99l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Vocab to Ids for all features\n",
        "# Given the feature, it extracts all unique values and assigns an id to each one.\n",
        "def vocab2id_builder(dataset, k):\n",
        "\n",
        "  items = []\n",
        "  for sentence in dataset:\n",
        "    for item in sentence[k]:\n",
        "      items.append(item)\n",
        "  \n",
        "  # Set '_' id to 0 for all Dict \n",
        "  items = set(items)\n",
        "  if '_' in items:\n",
        "    items.remove('_')\n",
        "    items = ['_'] + list(items)\n",
        "  else:\n",
        "    items = list(items)\n",
        "\n",
        "  items.append(UNK_TOKEN)\n",
        "  items.append(PAD_TOKEN)\n",
        "\n",
        "  vocab2id = {v:i for i, v in enumerate(items)}\n",
        "\n",
        "  return vocab2id"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHhJnvQ5Ni6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab2ids['lemmas'] = vocab2id_builder(train_single_pred_sentences, 'lemmas')\n",
        "vocab2ids['pos_tags'] = vocab2id_builder(train_single_pred_sentences, 'pos_tags')\n",
        "vocab2ids['predicate'] = vocab2id_builder(train_single_pred_sentences, 'predicate')\n",
        "vocab2ids['roles'] = vocab2id_builder(train_single_pred_sentences, 'roles')\n",
        "vocab2ids['predicate_indicator'] = vocab2id_builder(train_single_pred_sentences, 'predicate_indicator')\n",
        "vocab2ids['lemmas_indicator'] = vocab2id_builder(train_single_pred_sentences, 'lemmas_indicator')\n",
        "vocab2ids['bi_roles'] = vocab2id_builder(train_single_pred_sentences, 'bi_roles')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaGmaW2hDmbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the Dictionary\n",
        "import pickle\n",
        "\n",
        "# with open('/content/drive/My Drive/NLP_HW2/vocab2ids_final_v2', 'wb') as fp:\n",
        "#   pickle.dump(vocab2ids, fp)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3azxg3rAath3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the Dictionary\n",
        "with open ('/content/drive/My Drive/NLP_HW2/vocab2ids_v1', 'rb') as fp:\n",
        "  vocab2ids = pickle.load(fp)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI3OLYiwr7Gw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create id 2 label dictionary for 'roles'\n",
        "# Makes the decoding procedure more easy and faster \n",
        "id2class = {v: k for k, v in vocab2ids['roles'].items()}"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDmALWE4IsoM",
        "colab_type": "text"
      },
      "source": [
        "##### Read / Create Glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rq3U4onOIvU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = []\n",
        "id = 0\n",
        "word2id = {}\n",
        "vectors = []\n",
        "\n",
        "with open(glove_path, 'rb') as f:\n",
        "    for l in f:\n",
        "        line = l.decode().split()\n",
        "        word = line[0]\n",
        "        words.append(word)\n",
        "        word2id[word] = id\n",
        "        id += 1\n",
        "        vect = np.array(line[1:]).astype(np.float)\n",
        "        vectors.append(vect)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NooIOaF5IxAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove = {w: vectors[word2id[w]] for w in words}"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC9OUq-zQvT5",
        "colab_type": "text"
      },
      "source": [
        "### Create Windows / Batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zjl3VP89CiA1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check whether the item is in the Dictionary or not\n",
        "# If the item is not in the Dict, replace it the unknown token\n",
        "def vocab_checker(vocab2id_k, item):\n",
        "  if item in vocab2id_k:\n",
        "    return vocab2id_k[item]\n",
        "  else:\n",
        "    return vocab2id_k[UNK_TOKEN]"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ze5Iv6fQyVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "We don't use Torch DataLoader\n",
        "This function creates batches, adds padding and returns the max length in all batch\n",
        "Input: for each sentence we have different features (depends on Test / Train-Dev)\n",
        "Instead of \n",
        "          sentence1 = {'feature1', 'feature2',...}\n",
        "          sentence1 = {'feature1', 'feature2',...}\n",
        "\n",
        "We create: \n",
        "          feature1 = {'sentence1', 'sentence2',...} \n",
        "          feature2 = {'sentence1', 'sentence2',...}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def create_batches(dataset, batch_size):\n",
        "  batched_dataset = []\n",
        "  max_sent_len = []\n",
        "\n",
        "  for i in range(0, len(dataset), batch_size):\n",
        "    batch = dataset[i:i+batch_size]\n",
        "    keyed_batch = {}\n",
        "    for k in dataset[0].keys():\n",
        "      x = [sentence[k] for sentence in batch]\n",
        "      if k == 'sentence_id' or k == 'position_predicate':\n",
        "        keyed_batch[k] = x\n",
        "        continue\n",
        "      max_len = max([len(xx) for xx in x])\n",
        "      max_sent_len.append(max_len)\n",
        "      x = [xx + [PAD_TOKEN]*(max_len - len(xx)) for xx in x]\n",
        "      keyed_batch[k] = torch.tensor(\n",
        "          [[vocab_checker(vocab2ids[k], xxx) for xxx in xx] for xx in x]).to(device)\n",
        "    batched_dataset.append(keyed_batch)\n",
        "\n",
        "  return batched_dataset, max(max_sent_len)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98ApNzgYcPrk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_batched, max_sent_len_train = create_batches(train_single_pred_sentences, 64)\n",
        "dev_batched, max_sent_len_dev = create_batches(dev_single_pred_sentences, 64)\n",
        "test_batched, max_sent_len_test = create_batches(test_single_pred_sentences, 64)\n",
        "\n",
        "# We need max sentence length to build the positional encoder\n",
        "max_len_all = max([max_sent_len_train, max_sent_len_dev, max_sent_len_test])"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn1F_DhJzLyf",
        "colab_type": "text"
      },
      "source": [
        "###Create Dictionary / Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8DMTUh_5Pr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_weights = {\n",
        "    'lemmas':{},\n",
        "    'pos_tags':{},\n",
        "    'predicates': {},\n",
        "    'predicate_indicator': {},\n",
        "    'lemmas_indicator': {}\n",
        "}"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNc2TUHFzPf6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create weights for different features\n",
        "# Used Glove for 'lemmas'\n",
        "# For other features, randomly generate weights\n",
        "def embedding_weights_creator(glove, vocab2ids, feature):\n",
        "  matrix_len = len(vocab2ids[feature])\n",
        "  weights_matrix = np.zeros((matrix_len, 50))\n",
        "\n",
        "  for k, v in vocab2ids[feature].items():\n",
        "    if feature == 'lemmas':\n",
        "      try: \n",
        "        weights_matrix[v] = glove[k]\n",
        "      except KeyError:\n",
        "        weights_matrix[v] = np.random.normal(scale=0.6, size=(50, ))\n",
        "    else:\n",
        "      weights_matrix[v] = np.random.normal(scale=0.6, size=(50, ))\n",
        "      \n",
        "\n",
        "  return torch.tensor(weights_matrix, dtype=torch.float32) #to(device)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pO9dWvY7aCM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_weights['lemmas'] = embedding_weights_creator(glove, vocab2ids, 'lemmas')\n",
        "embedding_weights['pos_tags'] = embedding_weights_creator(glove, vocab2ids, 'pos_tags')\n",
        "embedding_weights['predicates'] = embedding_weights_creator(glove, vocab2ids, 'predicate')\n",
        "embedding_weights['predicate_indicator'] = embedding_weights_creator(glove, vocab2ids, 'predicate_indicator')\n",
        "embedding_weights['lemmas_indicator'] = embedding_weights_creator(glove, vocab2ids, 'lemmas_indicator')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKqCFwQtSQZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load previously build Dictionary\n",
        "with open ('/content/drive/My Drive/NLP_HW2/embedding_weights_v2', 'rb') as fp:\n",
        "  embedding_weights = pickle.load(fp)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX8WiOuhw8cS",
        "colab_type": "text"
      },
      "source": [
        "#### Saving Dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_alR_KNw-iB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "15f682b2-34fe-44f6-8b11-bc462b0ca218"
      },
      "source": [
        "# Save built dictionary\n",
        "# with open('/content/drive/My Drive/NLP_HW2/embedding_weights_final_v2', 'wb') as fp:\n",
        "#   pickle.dump(embedding_weights, fp)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
            "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X565sXx1vuhh",
        "colab_type": "text"
      },
      "source": [
        "### Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyBSCDTIgHMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "# Transformer works in parallel\n",
        "# Positional Encoder will help the model to take benefit from words position in sentences\n",
        "# Works based on Original Paper Attention is All you need and PyTorch Doc\n",
        "# Used Sine and Cosine for different positions\n",
        "# Max length is the longest sentence in datasets\n",
        "# dimention models is the embedding layers shape\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model=250, max_len=143):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        \n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1).to(device)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.shape[0]]\n",
        "        return x"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkGLnFuNU-y6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransEncoder(nn.Module):\n",
        "\n",
        "  def __init__(self, hparams):\n",
        "    super(TransEncoder, self).__init__()\n",
        "\n",
        "    self.pos_encoder = PositionalEncoding()\n",
        "\n",
        "    # Loads weights from previosuly generated weights\n",
        "    # But it will updates the weights during training\n",
        "    self.lemmas_embedding = nn.Embedding.from_pretrained(hparams.embedding_weights['lemmas'],freeze=False)\n",
        "    self.pos_tags_embedding = nn.Embedding.from_pretrained(hparams.embedding_weights['pos_tags'],freeze=False)\n",
        "    self.predicate_embedding = nn.Embedding.from_pretrained(hparams.embedding_weights['predicates'],freeze=False)\n",
        "    self.predicate_flag_embedding = nn.Embedding.from_pretrained(hparams.embedding_weights['predicate_indicator'],freeze=False)\n",
        "    self.lemmas_flag_embedding = nn.Embedding.from_pretrained(hparams.embedding_weights['lemmas_indicator'],freeze=False)\n",
        "\n",
        "    encoder_layers = nn.TransformerEncoderLayer(hparams.dim_emb,\n",
        "                                                hparams.num_heads,\n",
        "                                                hparams.dim_feedforward,\n",
        "                                                hparams.dropout)\n",
        "    self.transformer_encoder = nn.TransformerEncoder(encoder_layers, hparams.nlayers)\n",
        "\n",
        "    # Create two seperate linear layers \n",
        "    # First one is for binary labels\n",
        "    # Second one is for original labels - 36 in total\n",
        "    self.classifier_s3 = nn.Linear(hparams.dim_emb, hparams.num_classes_s3)\n",
        "    self.classifier_s4 = nn.Linear(hparams.dim_emb, hparams.num_classes_s4)\n",
        "\n",
        "    \n",
        "  def forward(self, src):\n",
        "\n",
        "    lemmas  = src['lemmas']\n",
        "    pos_tags = src['pos_tags']\n",
        "    predicates = src['predicates']\n",
        "    predicates_indicator = src['predicate_indicator']\n",
        "    lemmas_indicator = src['lemmas_indicator']\n",
        "    \n",
        "    lemmas_emb = self.lemmas_embedding(lemmas)\n",
        "    pos_emb = self.pos_tags_embedding(pos_tags)\n",
        "    pred_emb = self.predicate_embedding(predicates)\n",
        "    pred_flag_emb = self.predicate_flag_embedding(predicates_indicator)\n",
        "    lemmas_flag_emb = self.lemmas_flag_embedding(lemmas_indicator)\n",
        "\n",
        "\n",
        "    # Concat all 5 features' embedding weights\n",
        "    # Each one has 50 dim, so 250 in total\n",
        "    embeddings = torch.cat((lemmas_emb, pos_emb, pred_emb, pred_flag_emb, lemmas_flag_emb), -1)\n",
        "    embeddings = torch.transpose(embeddings, 0, 1)\n",
        "\n",
        "    # Add positional embedding\n",
        "    # the positional weights sums with the input embedding, so the output size would be the same as input\n",
        "    embeddings = self.pos_encoder(embeddings)\n",
        "\n",
        "    o = self.transformer_encoder(embeddings)\n",
        "\n",
        "    output_s3 = self.classifier_s3(o)\n",
        "    output_s4 = self.classifier_s4(o)\n",
        "    output_s3 = torch.transpose(output_s3, 0, 1)\n",
        "    output_s4 = torch.transpose(output_s4, 0, 1)\n",
        "\n",
        "    return output_s3, output_s4"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu91YnvFVBta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HParams():\n",
        "\n",
        "  dim_feedforward = 1024\n",
        "  dim_emb = 250\n",
        "  dropout = 0.2\n",
        "  nlayers = 6\n",
        "  num_heads = 10\n",
        "  num_classes_s3 = 4\n",
        "  num_classes_s4 = len(vocab2ids['roles'])\n",
        "  embedding_weights = embedding_weights\n",
        "\n",
        "params = HParams()"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCBUClKQ7u-h",
        "colab_type": "text"
      },
      "source": [
        "### Trainer Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdNIWsOb7whZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Trainer():\n",
        "\n",
        "  def __init__(\n",
        "    self,\n",
        "    model: nn.Module,\n",
        "    loss_function_s3,\n",
        "    loss_function_s4,\n",
        "    optimizer):\n",
        "\n",
        "    self.model = model\n",
        "    self.loss_function_s3 = loss_function_s3\n",
        "    self.loss_function_s4 = loss_function_s4\n",
        "    self.optimizer = optimizer\n",
        "\n",
        "  def train(self, train_dataset, \n",
        "            valid_dataset, \n",
        "            epochs):\n",
        "\n",
        "    train_loss = 0.0\n",
        "    for epoch in range(epochs):\n",
        "      print(f'Epoch {epoch+1}')\n",
        "\n",
        "      epoch_loss = 0.0\n",
        "      self.model.train()\n",
        "\n",
        "      for step, sentence in enumerate(train_dataset):\n",
        "\n",
        "        tokens = {\n",
        "            'lemmas': sentence['lemmas'],\n",
        "            'pos_tags': sentence['pos_tags'],\n",
        "            'predicates': sentence['predicate'],\n",
        "            'predicate_indicator': sentence['predicate_indicator'],\n",
        "            'lemmas_indicator': sentence['lemmas_indicator']\n",
        "        }\n",
        "        labels = {'roles': sentence['roles'],\n",
        "                  'bi_roles': sentence['bi_roles']}\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        predictions_s3, predictions_s4 = self.model(tokens)\n",
        "\n",
        "        # predictions_s3 = predictions_s3.reshape(-1, predictions_s3.shape[-1])\n",
        "        # predictions_s4 = predictions_s4.reshape(-1, predictions_s4.shape[-1])\n",
        "        # labels['roles'] = labels['roles'].view(-1)\n",
        "\n",
        "        predictions_s3 = torch.transpose(predictions_s3, 1, 2)\n",
        "        predictions_s4 = torch.transpose(predictions_s4, 1, 2)\n",
        "\n",
        "        temp_loss_s3 = self.loss_function_s3(predictions_s3, labels['bi_roles'])\n",
        "        temp_loss_s4 = self.loss_function_s4(predictions_s4, labels['roles'])\n",
        "\n",
        "        # Set loss for '_' tokens to zero\n",
        "        # So during backward, the model focuses on non '_' tokens\n",
        "        temp_loss_s4 = temp_loss_s4 * labels['bi_roles']\n",
        "\n",
        "        temp_loss_s3 = temp_loss_s3.mean(dim=-1).mean()\n",
        "        temp_loss_s4 = temp_loss_s4.mean(dim=-1).mean()\n",
        "\n",
        "        temp_loss = temp_loss_s3 + temp_loss_s4\n",
        "\n",
        "        temp_loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        epoch_loss += temp_loss.tolist()\n",
        "\n",
        "        \n",
        "      avg_epoch_loss = epoch_loss / len(train_dataset)\n",
        "      train_loss += avg_epoch_loss\n",
        "      print(f'\\t[Epoch: {epoch+1}] Training Loss = {avg_epoch_loss}')\n",
        "\n",
        "      valid_loss = self.evaluate(valid_dataset)\n",
        "      print(f'\\t[Epoch: {epoch+1}] Validation Loss = {valid_loss}')\n",
        "\n",
        "    print('Training has finished')\n",
        "    \n",
        "    avg_epoch_loss = train_loss / epochs\n",
        "    return avg_epoch_loss\n",
        "  \n",
        "\n",
        "  def evaluate(self, valid_dataset):\n",
        "\n",
        "    valid_loss = 0.0\n",
        "    self.model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for sentence in valid_dataset:\n",
        "        tokens = {\n",
        "          'lemmas': sentence['lemmas'],\n",
        "          'pos_tags': sentence['pos_tags'],\n",
        "          'predicates': sentence['predicate'],\n",
        "          'predicate_indicator': sentence['predicate_indicator'],\n",
        "          'lemmas_indicator': sentence['lemmas_indicator']\n",
        "\n",
        "        }\n",
        "        labels = {'roles': sentence['roles'],\n",
        "                  'bi_roles': sentence['bi_roles']}\n",
        "\n",
        "        predictions_s3, predictions_s4 = self.model(tokens)\n",
        "\n",
        "        # predictions_s3 = predictions_s3.reshape(-1, predictions_s3.shape[-1])\n",
        "        # predictions_s4 = predictions_s4.reshape(-1, predictions_s4.shape[-1])\n",
        "        # labels['roles'] = labels['roles'].view(-1)\n",
        "\n",
        "        predictions_s3 = torch.transpose(predictions_s3, 1, 2)\n",
        "        predictions_s4 = torch.transpose(predictions_s4, 1, 2)\n",
        "\n",
        "        temp_loss_s3 = self.loss_function_s3(predictions_s3, labels['bi_roles'])\n",
        "        temp_loss_s4 = self.loss_function_s4(predictions_s4, labels['roles'])\n",
        "\n",
        "        # Set loss for '_' tokens to zero\n",
        "        # So during backward, the model focuses on non '_' tokens\n",
        "        temp_loss_s4 = temp_loss_s4 * labels['bi_roles']\n",
        "\n",
        "        temp_loss_s3 = temp_loss_s3.mean(dim=-1).mean()\n",
        "        temp_loss_s4 = temp_loss_s4.mean(dim=-1).mean()\n",
        "\n",
        "        temp_loss = temp_loss_s3 + temp_loss_s4\n",
        "\n",
        "        valid_loss += temp_loss.tolist()\n",
        "      \n",
        "    return valid_loss / len(valid_dataset)\n",
        "\n",
        "\n",
        "  def predict(self, x):\n",
        "\n",
        "    self.model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      logits_s3, logits_s4 = self.model(x)\n",
        "      predictions_s3 = torch.argmax(logits_s3, -1)\n",
        "      predictions_s4 = torch.argmax(logits_s4, -1)\n",
        "      predictions = predictions_s3 * predictions_s4\n",
        "      return predictions"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJZb8f8S77tv",
        "colab_type": "text"
      },
      "source": [
        "### Transformer Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fbzx4JLV8UKn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pprint import pprint"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVPKBAcE7-JT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "28dc8679-2573-4387-a1ed-567d70c59c9e"
      },
      "source": [
        "model = TransEncoder(params).cuda()\n",
        "model"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TransEncoder(\n",
              "  (pos_encoder): PositionalEncoding()\n",
              "  (lemmas_embedding): Embedding(27349, 50)\n",
              "  (pos_tags_embedding): Embedding(50, 50)\n",
              "  (predicate_embedding): Embedding(458, 50)\n",
              "  (predicate_flag_embedding): Embedding(4, 50)\n",
              "  (lemmas_flag_embedding): Embedding(272, 50)\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): Linear(in_features=250, out_features=250, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=250, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "        (linear2): Linear(in_features=1024, out_features=250, bias=True)\n",
              "        (norm1): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.2, inplace=False)\n",
              "        (dropout2): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (1): TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): Linear(in_features=250, out_features=250, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=250, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "        (linear2): Linear(in_features=1024, out_features=250, bias=True)\n",
              "        (norm1): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.2, inplace=False)\n",
              "        (dropout2): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (2): TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): Linear(in_features=250, out_features=250, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=250, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "        (linear2): Linear(in_features=1024, out_features=250, bias=True)\n",
              "        (norm1): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.2, inplace=False)\n",
              "        (dropout2): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (3): TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): Linear(in_features=250, out_features=250, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=250, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "        (linear2): Linear(in_features=1024, out_features=250, bias=True)\n",
              "        (norm1): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.2, inplace=False)\n",
              "        (dropout2): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (4): TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): Linear(in_features=250, out_features=250, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=250, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "        (linear2): Linear(in_features=1024, out_features=250, bias=True)\n",
              "        (norm1): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.2, inplace=False)\n",
              "        (dropout2): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (5): TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): Linear(in_features=250, out_features=250, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=250, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "        (linear2): Linear(in_features=1024, out_features=250, bias=True)\n",
              "        (norm1): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.2, inplace=False)\n",
              "        (dropout2): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier_s3): Linear(in_features=250, out_features=4, bias=True)\n",
              "  (classifier_s4): Linear(in_features=250, out_features=36, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rW7jJiYB8eka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    loss_function_s3 = nn.CrossEntropyLoss(ignore_index=vocab2ids['bi_roles'][PAD_TOKEN],  reduction='none'),\n",
        "    loss_function_s4 = nn.CrossEntropyLoss(ignore_index=vocab2ids['roles'][PAD_TOKEN],  reduction='none'),\n",
        "    optimizer = optim.Adam(model.parameters()),\n",
        ")"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LH0fh5GGkpkO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "1b32fd8d-90b3-4aa5-edba-a656e5046776"
      },
      "source": [
        "trainer.train(train_batched, dev_batched, 10)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "\t[Epoch: 1] Training Loss = 0.03601293944869955\n",
            "\t[Epoch: 1] Validation Loss = 0.04560981384095024\n",
            "Epoch 2\n",
            "\t[Epoch: 2] Training Loss = 0.04052025884512807\n",
            "\t[Epoch: 2] Validation Loss = 0.045496053993701935\n",
            "Epoch 3\n",
            "\t[Epoch: 3] Training Loss = 0.03509779072432128\n",
            "\t[Epoch: 3] Validation Loss = 0.04411199320034653\n",
            "Epoch 4\n",
            "\t[Epoch: 4] Training Loss = 0.043398586890180536\n",
            "\t[Epoch: 4] Validation Loss = 0.04752588900280934\n",
            "Epoch 5\n",
            "\t[Epoch: 5] Training Loss = 0.03709767724239031\n",
            "\t[Epoch: 5] Validation Loss = 0.04726496368062263\n",
            "Epoch 6\n",
            "\t[Epoch: 6] Training Loss = 0.033151667599180125\n",
            "\t[Epoch: 6] Validation Loss = 0.04589682565454174\n",
            "Epoch 7\n",
            "\t[Epoch: 7] Training Loss = 0.034131137441063414\n",
            "\t[Epoch: 7] Validation Loss = 0.04580041620076871\n",
            "Epoch 8\n",
            "\t[Epoch: 8] Training Loss = 0.034380069921806006\n",
            "\t[Epoch: 8] Validation Loss = 0.04861097227708966\n",
            "Epoch 9\n",
            "\t[Epoch: 9] Training Loss = 0.03427684732607247\n",
            "\t[Epoch: 9] Validation Loss = 0.04566615195397068\n",
            "Epoch 10\n",
            "\t[Epoch: 10] Training Loss = 0.033483985885630334\n",
            "\t[Epoch: 10] Validation Loss = 0.04790566609624554\n",
            "Training has finished\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.036155096132447204"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TmRaZw_322-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_save_name = 'Transformer_Glove_2Step_30E_F1024_L6_H10_Em250_D2_relu_final_v2.pth'\n",
        "path = f\"/content/drive/My Drive/NLP_HW2/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gd4c3nYk6aB",
        "colab_type": "text"
      },
      "source": [
        "#### Loading Best Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wppkxnevjirf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "96ae3c21-672f-4a9c-ad9e-552776829b42"
      },
      "source": [
        "# load model\n",
        "srl_model = TransEncoder(hparams=params).cuda()\n",
        "srl_model.load_state_dict(torch.load('/content/drive/My Drive/NLP_HW2/Transformer_Glove_2Step_30E_F1024_L6_H10_Em250_D2_relu.pth'))\n",
        "srl_model.eval()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TransEncoder(\n",
              "  (pos_encoder): PositionalEncoding()\n",
              "  (lemmas_embedding): Embedding(27349, 50)\n",
              "  (pos_tags_embedding): Embedding(50, 50)\n",
              "  (predicate_embedding): Embedding(458, 50)\n",
              "  (predicate_flag_embedding): Embedding(4, 50)\n",
              "  (lemmas_flag_embedding): Embedding(272, 50)\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): Linear(in_features=250, out_features=250, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=250, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "        (linear2): Linear(in_features=1024, out_features=250, bias=True)\n",
              "        (norm1): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.2, inplace=False)\n",
              "        (dropout2): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (1): TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): Linear(in_features=250, out_features=250, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=250, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "        (linear2): Linear(in_features=1024, out_features=250, bias=True)\n",
              "        (norm1): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.2, inplace=False)\n",
              "        (dropout2): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (2): TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): Linear(in_features=250, out_features=250, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=250, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "        (linear2): Linear(in_features=1024, out_features=250, bias=True)\n",
              "        (norm1): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.2, inplace=False)\n",
              "        (dropout2): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (3): TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): Linear(in_features=250, out_features=250, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=250, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "        (linear2): Linear(in_features=1024, out_features=250, bias=True)\n",
              "        (norm1): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.2, inplace=False)\n",
              "        (dropout2): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (4): TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): Linear(in_features=250, out_features=250, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=250, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "        (linear2): Linear(in_features=1024, out_features=250, bias=True)\n",
              "        (norm1): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.2, inplace=False)\n",
              "        (dropout2): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (5): TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): Linear(in_features=250, out_features=250, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=250, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "        (linear2): Linear(in_features=1024, out_features=250, bias=True)\n",
              "        (norm1): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.2, inplace=False)\n",
              "        (dropout2): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier_s3): Linear(in_features=250, out_features=4, bias=True)\n",
              "  (classifier_s4): Linear(in_features=250, out_features=36, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hgh3Ofwok9sW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = Trainer(\n",
        "    model = srl_model,\n",
        "    loss_function_s3 = nn.CrossEntropyLoss(ignore_index=vocab2ids['bi_roles'][PAD_TOKEN],  reduction='none'),\n",
        "    loss_function_s4 = nn.CrossEntropyLoss(ignore_index=vocab2ids['roles'][PAD_TOKEN],  reduction='none'),\n",
        "    optimizer = optim.Adam(srl_model.parameters()),\n",
        ")"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HX_5Gw5AuBHY",
        "colab_type": "text"
      },
      "source": [
        "#### Test Dataset Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKyNLE_VwAYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_predictions = []\n",
        "test_others = []\n",
        "\n",
        "for sentence in test_batched:\n",
        "  tokens = {\n",
        "    'lemmas': sentence['lemmas'],\n",
        "    'pos_tags': sentence['pos_tags'],\n",
        "    'predicates': sentence['predicate'],\n",
        "    'predicate_indicator': sentence['predicate_indicator'],\n",
        "    'lemmas_indicator': sentence['lemmas_indicator']\n",
        "  }\n",
        "  others = {\n",
        "    'sentence_id': sentence['sentence_id'],\n",
        "    'position_predicate': sentence['position_predicate']\n",
        "  }\n",
        "\n",
        "  predicts = trainer.predict(tokens)\n",
        "  test_predictions.append(predicts)\n",
        "  test_others.append(others)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ6rsEuPtQ6u",
        "colab_type": "text"
      },
      "source": [
        "### Decoder / Merger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWnCZRa_xqdT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extracting Sentence IDs and Predicate Positions\n",
        "sentence_id = [sent_id['sentence_id'] for sent_id in test_others]\n",
        "position_predicate = [batch['position_predicate'] for batch in test_others]"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGUfoJKpQv5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "position_pred_flatten = [xx for x in position_predicate for xx in x]\n",
        "pred_flatten = [xx for x in test_predictions for xx in x]\n",
        "sentence_id_flatten = [xx for x in sentence_id for xx in x]"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0-ad6BwQ7JX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_sentence_id = [sent_id for sent_id in test_sentences]\n",
        "unique_sent_id = list(set(all_sentence_id))"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAfqC6_P9vHX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sentence Merger\n",
        "roles_prediction = {}\n",
        "\n",
        "for id in unique_sent_id:\n",
        "  tmp_dict = {}\n",
        "  for i, item in enumerate(pred_flatten):\n",
        "    if id == sentence_id_flatten[i]: # if 'roles' is not empty, at least one predicate\n",
        "      tmp_dict[position_pred_flatten[i]] = pred_flatten[i]\n",
        "\n",
        "  roles_prediction[id] = {\n",
        "      'roles' : tmp_dict\n",
        "  }"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx4NGWugvDGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sentence Decoder\n",
        "def sentence_decoder(sentences, predictions, id2vocabs):\n",
        "  for sentence_id in sentences:\n",
        "    pred = predictions[sentence_id]['roles']\n",
        "    lemmas_len = len(sentences[sentence_id]['words'])  # original sentence length\n",
        "    for idx in pred.keys():\n",
        "      pred[idx] = pred[idx][:lemmas_len]\n",
        "      decoded_sentence = [id2vocabs[item.item()] for item in pred[idx]]\n",
        "      pred[idx] = decoded_sentence\n",
        "        \n",
        "  return predictions"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm68o-5i0Lsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoded_predictions = sentence_decoder(test_sentences, roles_prediction, id2class)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3Cpa0mLYM-y",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frgai_ShYQFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_argument_identification(labels, predictions, null_tag='_'):\n",
        "    true_positives, false_positives, false_negatives = 0, 0, 0\n",
        "    for sentence_id in labels:\n",
        "        gold = labels[sentence_id]['roles']\n",
        "        pred = predictions[sentence_id]['roles']\n",
        "        predicate_indices = set(gold.keys()).union(pred.keys())\n",
        "        for idx in predicate_indices:\n",
        "            if idx in gold and idx not in pred:\n",
        "                false_negatives += sum(1 for role in gold[idx] if role != null_tag)\n",
        "            elif idx in pred and idx not in gold:\n",
        "                false_positives += sum(1 for role in pred[idx] if role != null_tag)\n",
        "            else: # idx in both gold and pred\n",
        "                for r_g, r_p in zip(gold[idx], pred[idx]):\n",
        "                    if r_g != null_tag and r_p != null_tag:\n",
        "                        true_positives += 1\n",
        "                    elif r_g != null_tag and r_p == null_tag:\n",
        "                        false_negatives += 1\n",
        "                    elif r_g == null_tag and r_p != null_tag:\n",
        "                        false_positives += 1\n",
        "\n",
        "    precision = true_positives / (true_positives + false_positives)\n",
        "    recall = true_positives / (true_positives + false_negatives)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    return {\n",
        "        'true_positives': true_positives,\n",
        "        'false_positives': false_positives,\n",
        "        'false_negatives': false_negatives,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }\n",
        "\n",
        "\n",
        "def evaluate_argument_classification(labels, predictions, null_tag='_'):\n",
        "    true_positives, false_positives, false_negatives = 0, 0, 0\n",
        "    for sentence_id in labels:\n",
        "        gold = labels[sentence_id]['roles']\n",
        "        pred = predictions[sentence_id]['roles']\n",
        "        predicate_indices = set(gold.keys()).union(pred.keys())\n",
        "\n",
        "        for idx in predicate_indices:\n",
        "            if idx in gold and idx not in pred:\n",
        "                false_negatives += sum(1 for role in gold[idx] if role != null_tag)\n",
        "            elif idx in pred and idx not in gold:\n",
        "                false_positives += sum(1 for role in pred[idx] if role != null_tag)\n",
        "            else: # idx in both gold and pred\n",
        "                for r_g, r_p in zip(gold[idx], pred[idx]):\n",
        "                    if r_g != null_tag and r_p != null_tag:\n",
        "                        if r_g == r_p:\n",
        "                            true_positives += 1\n",
        "                        else:\n",
        "                            false_positives += 1\n",
        "                            false_negatives += 1\n",
        "                    elif r_g != null_tag and r_p == null_tag:\n",
        "                        false_negatives += 1\n",
        "                    elif r_g == null_tag and r_p != null_tag:\n",
        "                        false_positives += 1\n",
        "                        \n",
        "    precision = true_positives / (true_positives + false_positives)\n",
        "    recall = true_positives / (true_positives + false_negatives)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    return {\n",
        "        'true_positives': true_positives,\n",
        "        'false_positives': false_positives,\n",
        "        'false_negatives': false_negatives,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy2I27UKr612",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _get_table_line(a, b, c):\n",
        "    if isinstance(b, float):\n",
        "        b = '{:0.2f}'.format(b)\n",
        "    if isinstance(c, float):\n",
        "        c = '{:0.2f}'.format(c)\n",
        "\n",
        "    line = '{:^20}|{:^20}|{:^20}'.format(a, b, c)\n",
        "    return line\n",
        "\n",
        "def print_table(title, results):\n",
        "    header = _get_table_line('', 'Gold Positive', 'Gold Negative')\n",
        "    header_sep = '=' * len(header)\n",
        "\n",
        "    first_line = _get_table_line('Pred Positive', results['true_positives'], results['false_positives'])\n",
        "    second_line = _get_table_line('Pred Negative', results['false_negatives'], '')\n",
        "\n",
        "    precision = 'Precision = {:0.4f}'.format(results['precision'])\n",
        "    recall = 'Recall    = {:0.4f}'.format(results['recall'])\n",
        "    f1 = 'F1 score  = {:0.4f}'.format(results['f1'])\n",
        "\n",
        "    output = '{}\\n\\n{}\\n{}\\n{}\\n{}\\n\\n\\n{}\\n{}\\n{}\\n\\n\\n'.format(title.upper(), header, header_sep, first_line, second_line, precision, recall, f1)\n",
        "    return output"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGEIHNc8YZBK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "c8325443-0094-42f1-aab9-0d8017479af6"
      },
      "source": [
        "    print('MODEL: ARGUMENT IDENTIFICATION + ARGUMENT CLASSIFICATION')\n",
        "    argument_identification_results = evaluate_argument_identification(test_labels, decoded_predictions)\n",
        "    argument_classification_results = evaluate_argument_classification(test_labels, decoded_predictions)\n",
        "    print(print_table('argument identification', argument_identification_results))\n",
        "    print(print_table('argument classification', argument_classification_results))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MODEL: ARGUMENT IDENTIFICATION + ARGUMENT CLASSIFICATION\n",
            "ARGUMENT IDENTIFICATION\n",
            "\n",
            "                    |   Gold Positive    |   Gold Negative    \n",
            "==============================================================\n",
            "   Pred Positive    |       10309        |        991         \n",
            "   Pred Negative    |        1144        |                    \n",
            "\n",
            "\n",
            "Precision = 0.9123\n",
            "Recall    = 0.9001\n",
            "F1 score  = 0.9062\n",
            "\n",
            "\n",
            "\n",
            "ARGUMENT CLASSIFICATION\n",
            "\n",
            "                    |   Gold Positive    |   Gold Negative    \n",
            "==============================================================\n",
            "   Pred Positive    |        9754        |        1546        \n",
            "   Pred Negative    |        1699        |                    \n",
            "\n",
            "\n",
            "Precision = 0.8632\n",
            "Recall    = 0.8517\n",
            "F1 score  = 0.8574\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}